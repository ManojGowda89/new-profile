# robots.txt for https://profile.manojgowda.in
# General instructions for all search engines

User-agent: *
Disallow: 

# Allow all search engines to crawl everything
Allow: /

# Sitemap locations for multiple domains
Sitemap: https://profile.manojgowda.in/sitemap.xml

# Crawl-delay directive (optional) to slow down crawling speed for heavy sites
# Crawl-delay: 10   - wait 10 seconds between requests

# SEO-specific directives (such as for Googlebot)
User-agent: Googlebot
Disallow: /private/  # Disallow access to sensitive/private sections for Googlebot
Allow: /public/      # Explicitly allow crawling of public sections

# Specific user-agent exclusions (if you have any)
User-agent: BadBot
Disallow: /  # Block specific bots that are problematic

# Include robots meta tags in HTML files for finer control over individual pages
# Example: <meta name="robots" content="noindex, nofollow">
